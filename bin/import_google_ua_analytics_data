#!/usr/bin/env ruby
require 'logger'
require 'optparse'
require 'google/apis/analytics_v3'
require 'google/api_client/auth/key_utils'
require_relative '../lib/ncbo_cron/analytics/object_analytics_job'

module NcboCron
  module Models

    class OntologyVisitsAnalytics
      def fetch_ua_object_analytics(logger, ua_conn)
        @logger = logger
        @ua_conn = ua_conn
        aggregated_results = Hash.new
        start_year = @start_date.year || 2013
        filter_str = (@analytics_filter.nil? || @analytics_filter.empty?) ? "" : ";#{@analytics_filter}"

        @ont_acronyms.each do |acronym|
          max_results = 10000
          start_index = 1
          loop do
            results = @ua_conn.run_request(
              metrics: ['pageviews'],
              dimensions: %w[pagePath year month],
              filters: [['pagePath', "~^(\\/ontologies\\/#{acronym})(\\/?\\?{0}|\\/?\\?{1}.*)$#{filter_str}"]],
              start_index: start_index,
              max_results: max_results,
              dates_ranges: [@start_date, Date.today.to_s],
              sort: %w[year month]
            )
            results.rows ||= []
            start_index += max_results
            num_results = results.rows.length
            @logger.info "Acronym: #{acronym}, Results: #{num_results}, Start Index: #{start_index}"
            aggregated_results[acronym] = Hash.new unless aggregated_results.has_key?(acronym)
            aggregate_results(aggregated_results[acronym], results.rows)

            if num_results < max_results
              # fill up non existent years
              (start_year..Date.today.year).each do |y|
                aggregated_results[acronym] = Hash.new if aggregated_results[acronym].nil?
                aggregated_results[acronym][y.to_s] = Hash.new unless aggregated_results[acronym].has_key?(y.to_s)
                # fill up non existent months with zeros
                last_month = y.eql?(Date.today.year) ? Date.today.month : 12
                (1..last_month).each { |n|  aggregated_results[acronym][y.to_s][n.to_s] = 0 if aggregated_results[acronym][y.to_s].is_a?(Hash) && !aggregated_results[acronym][y.to_s].has_key?(n.to_s)}
              end
              break
            end
          end
        end
        sort_ga_data(aggregated_results)
      end
    end

    class UsersVisitsAnalytics
      def fetch_ua_object_analytics(logger, ua_conn)

        aggregated_results = Hash.new
        start_year = @start_date.year || 2013

        max_results = 10000
        start_index = 1
        loop do
          results = ua_conn.run_request(
            metrics: ['newUsers'],
            dimensions: %w[year month],
            filters: [],
            start_index: start_index,
            max_results: max_results,
            dates_ranges: [@start_date.to_s, Date.today.to_s],
            sort: %w[year month]
          )
          results.rows ||= []
          start_index += max_results
          num_results = results.rows.length
          logger.info "Results: #{num_results}, Start Index: #{start_index}"

          aggregate_results(aggregated_results, results.rows.map { |row| [-1] + row })

          if num_results < max_results
            # fill up non existent years
            (start_year..Date.today.year).each do |y|
              aggregated_results = Hash.new if aggregated_results.nil?
              aggregated_results[y.to_s] = Hash.new unless aggregated_results.has_key?(y.to_s)

              # fill up non existent months with zeros
              last_month = y.eql?(Date.today.year) ? Date.today.month.to_i : 12
              (1..last_month).each { |n|  aggregated_results[y.to_s][n.to_s] = 0 if aggregated_results[y.to_s].is_a?(Hash) && !aggregated_results[y.to_s].has_key?(n.to_s)}
            end

            break
          end
        end
        { "all_users" => aggregated_results }
      end
    end
  end
  module GoogleAnalyticsUAMigrator
    # Old version of Google Analytics connector
    class GoogleAnalyticsUAConnector
      def initialize(app_id:, app_name:, app_version:, analytics_key_file:, app_user:, start_date:, analytics_filter:)
        @app_id = app_id
        @app_name = app_name
        @app_version = app_version
        @analytics_key_file = analytics_key_file
        @app_user = app_user
        @generated_file_path = NcboCron.settings.analytics_path_to_ga_data_file
        @start_date = start_date
        @analytics_filter = analytics_filter
        @ga_client = authenticate_google
      end

      def run_request(metrics:, dimensions:, filters:, start_index:, max_results:, dates_ranges:, sort:)
        @ga_client.get_ga_data(
          ids = @app_id,
          start_date = dates_ranges.first,
          end_date = dates_ranges.last,
          metrics = metrics.map { |m| "ga:#{m}" }.join(','),
          {
            dimensions: dimensions.map { |d| "ga:#{d}" }.join(','),
            filters: filters.empty? ? nil : filters.map { |f, v| "ga:#{f}=#{v}" }.join(','),
            start_index: start_index,
            max_results: max_results,
            sort: sort.map { |d| "ga:#{d}" }.join(',')
          }
        )
      end

      private

      def authenticate_google
        Google::Apis::ClientOptions.default.application_name = @app_name
        Google::Apis::ClientOptions.default.application_version = @app_version
        # enable google api call retries in order to
        # minigate analytics processing failure due to occasional google api timeouts and other outages
        Google::Apis::RequestOptions.default.retries = 5
        # uncoment to enable logging for debugging purposes
        # Google::Apis.logger.level = Logger::DEBUG
        # Google::Apis.logger = @logger
        client = Google::Apis::AnalyticsV3::AnalyticsService.new
        key = Google::APIClient::KeyUtils::load_from_pkcs12(@analytics_key_file, 'notasecret')
        client.authorization = Signet::OAuth2::Client.new(
          :token_credential_uri => 'https://accounts.google.com/o/oauth2/token',
          :audience => 'https://accounts.google.com/o/oauth2/token',
          :scope => 'https://www.googleapis.com/auth/analytics.readonly',
          :issuer => @app_user,
          :signing_key => key
        ).tap { |auth| auth.fetch_access_token! }
        client
      end
    end

    def self.run(logger, options)
      @start_date = options[:start_date]
      @ua_conn = GoogleAnalyticsUAConnector.new(options)

      logger.info "Fetching UA analytics for all ontologies from #{@start_date} to today..."
      save = {}
      analytics_to_migrate = [NcboCron::Models::OntologyVisitsAnalytics,
                              NcboCron::Models::UsersVisitsAnalytics]
      analytics_to_migrate.each do |analytic_object|
        analytic_object = analytic_object.new(start_date: @start_date)
        ua_data = analytic_object.fetch_ua_object_analytics(logger, @ua_conn)
        save[analytic_object.redis_field] = ua_data
      end
      new_ga_start_date = NcboCron::Models::ObjectAnalytics::GA4_START_DATE
      NcboCron::Models::ObjectAnalyticsJob.new(logger).send(:save_data_in_file, save, new_ga_start_date)
      logger.info "Completed Universal Analytics pull..."
      logger.close
    end
  end
end
require 'bundler/setup'
require_relative '../lib/ncbo_cron'
require_relative '../config/config'


# # Google Analytics UA config
options = {
  app_id: nil,
  app_name: nil,
  app_version: nil,
  analytics_key_file: nil,
  app_user: nil,
  start_date: nil,
  analytics_filter: nil,
  logfile: nil
}

help_text = <<EOS
If you have existing Google Analytics UA data that you have been collecting till end 2023,
you can import it from Google (available till July 1st, 2024) so itâ€™s merged with
your Google Analytics Data (GA4) data and displayed in OntoPortal.
  
This script needs to be run ONLY ONCE, and the imported data will be used by your Google Analytics
CRON job to merge the 'old' data with the new data from GA4.
EOS

opt = OptionParser.new do |opts|
  opts.banner = "Usage: #{File.basename($PROGRAM_NAME)} [options]\n #{help_text}"
  opts.on('--app-id APP_ID', 'Application ID') { |v| options[:app_id] = v }
  opts.on('--app-name APP_NAME', 'Application Name') { |v| options[:app_name] = v }
  opts.on('--app-version APP_VERSION', 'Application Version') { |v| options[:app_version] = v }
  opts.on('--analytics-key-file KEY_FILE', 'Path to Analytics Key File') { |v| options[:analytics_key_file] = v }
  opts.on('--app-user APP_USER', 'Application User') { |v| options[:app_user] = v }
  opts.on('--start-date START_DATE', 'Start Date') { |v| options[:start_date] = v }
  opts.on('--analytics-filter FILTER', 'Analytics Filter String') { |v| options[:analytics_filter] = v }
  opts.on('--logfile LOGFILE', 'Path to Log File') { |v| options[:logfile] = v }
  opts.on_tail('-h', '--help', 'Show this help message') do
    puts opts
    exit
  end
end

begin
  opt.parse!
rescue OptionParser::MissingArgument, OptionParser::InvalidOption
  puts "Error: #{$!}"
  puts opt
  exit(1)
end

# Validate required options
required_options = %i[app_id app_name app_version analytics_key_file app_user start_date analytics_filter]
missing_options = required_options.select { |opt| options[opt].nil? }

if missing_options.any?
  puts "Missing required options: #{missing_options.join(', ')}"
  puts opt
  exit(1)
end

# Create a logger that writes to the specified logfile or STDOUT if not provided
logfile = options.delete(:logfile) || STDOUT
stdout_logger = Logger.new(logfile)

# Run the migration with the specified logger and options
NcboCron::GoogleAnalyticsUAMigrator.run(stdout_logger, options)